# 贪心算法

## 贪心算法原理
前一节中设计贪心算法的步骤比较繁琐，总结一下：
1. 确定问题的最优子结构
2. 设计一个递归算法
3. 证明如果每次都做贪心选择，那么就只剩下一个子问题
4. 证明贪心选择总是安全的
5. 设计递归实现贪心算法
6. 设计迭代实现贪心算法

更一般的，对于最优化问题我们可以这样得到贪心算法：
1. 将最优化问题转化为这样的形式：做出一次选择之后，只剩下一个子问题需要求解
2. 证明做出贪心选择之后，原问题总是存在最优解，即贪心选择选择的项总在某个最优解中
3. 证明做出贪心选择之后，剩余的子问题满足性质：其最优解与贪心选择组合即可得到原问题的最优解。

### 贪心选择性质
**贪心选择性质**：我们可以通过做出局部最优解选择来构造全局最优解。

### 最优子结构
如果一个问题的最优解包含其子问题的最优解，那么称此问题具有最优子结构性质。以活动选择问题为例，假设一个问题 $S_{ij}$ 的最优解包含活动 $a_{k}$ ，那么它必然包含子问题 $S_{ik}$ 和 $S_{kj}$ 的最优解。

对于动态规划问题，需要考虑所有 $a_{k}, k\in{i,...,j}$，得到$S_{ij}$ 的解。应用于贪心算法时，只证明：贪心选择得到的$a_{k}$ 与子问题的最优解节合就可得到原问题的最优解。

### 0-1 背包问题
研究一个经典最优化问题的两个变形。

**0-1 背包问题**：劫匪在商店发现了 n 个商品，第 i 个商品的价值为 $v_{i}$，重$w_{i}$，都是整数。小偷的背包总共可以放 W 总重的商品，问如何选择商品可以使得总价值最高。  

**分数背包问题**：其余要求都一样，但是小偷可以拿走商品的一部分。可以理解为商品变成了品质不同的金砂。

两个背包问题都具有最优子结构性质。对0-1背包问题，假设最优方案中包含第 i 个商品，那么剩余商品的组合必须是总重为 W - $w_{i}$ 的最优方案。分数背包问题类似。

但是这里的区别在于，对于分数背包问题，我们可以使用贪心算法。此时的贪心选择为小偷每次都是选择 每磅价值最高的商品，直到背包被装满。

## 赫夫曼编码

**前缀码**：即没有任何码字是其他码字的前缀。与任何字符编码相比，前缀码可以保证达到最优数据压缩率。

### 构造赫夫曼编码
### 赫夫曼算法的正确性

    引理 令 C 为一个字母表，其中每个字符 c 都有一个频率c.freq。令 x 和 y 是 C 中频率最低的两个字符。那么存在 C 的一个最优前缀码，x 和 y 的码字长度相同，且只有最后一个二进制位不同。

该引理说明，通过合并来构造最优树的过程，可以从合并出现频率最低的两个字符这样一个贪心选择开始。

## 贪心法的理论基础

一个**拟阵(Matriod)**是满足下列条件的一个有序对$M=(S,l)$  
1. S 是一个有穷非空集合
2. l 是 S 的一个非空子集族，称为 S 的独立子集，使得如果$B\in l$ 且 $A\subseteq B$，那么$A\in l$。我们说 l 是具有遗传性质的。
3. 如果 $A\in l$，$B\in l$，且 |A| < |B|，则有某个元素 $x\in B-A$ 使得 $A\cup{x}\in l$。我们称 M 满足交换性质。




## 任务调度问题

给定一个调度实现。在该策略中，如果某个任务在其 deadline 之后完成，就说它是迟的，否则就是早的。我们总是可以将任意一个调度实现转换称`early-first form`，在这种情况下，早任务在迟任务之前。解释原因，注意如果有某个早任务$a_{i}$跟在一个迟任务$a_{j}$之后，那么总是可以交换$a_{i}$和$a_{j}$的位置。


----
[When Greedy Algorithms are Perfect: the Matroid](https://jeremykun.com/2014/08/26/when-greedy-algorithms-are-perfect-the-matroid/https://jeremykun.com/2014/08/26/when-greedy-algorithms-are-perfect-the-matroid/)
## 最小生成树
对于无向图 $G=(V,E)$ 来说，一个常见问题是 find a connected subset of edges that touch every vertex. 

A minimal subset of edges in a backbone like this is guaranteed to form a tree. As such, these "backbone" are called spanning trees. "Span" means that you can get from ant vertex to another vertex.

When your edges $e\in E$ have nonnegative weights $w_{e}\in R^{>=0}$, we can further ask to find a minimum cost spanning tree.

**Definition** A minimum spanning tree T of a weighted graph G (with weights $w_{e}>=0$ for $e\in E$)is a spanning tree which minimizes the quantity
$$
w(T)=\sum_{e\in T} w_{e}
$$

对于寻找最小生成树问题来说有多种解法，这里介绍 Kruskal 的算法。

We'll maintain a forest F in G, which is just a subgraph consisting of a bunch of trees that may or may not be connected. At the beginning F is just all the vertices with no edges. And then at each step we add to F the edge e whose weight is smallest and also does not introduce any cycles into F. If the input graph G is connected then this will always produce a minimal spanning tree.

**Theorem** Kruskal's algorithm produces a minimal spanning tree of a connexted graph

**Proof**.

设 $F_{t}$ 是算法第 t 步产生的 forest。那么 $F_{0}$ 就是所有定点 G 的集合，$F_{n-1}$ 就是算法最后产生的答案（对于一个具有 n 个定点的图，其最小生成树中有 n-1 条边）。很明显 $F_{n-1}$ 是一棵树。所有具有 n-1 条边的树都是生成树。

现在需要证明 $F_{n-1}$ 具有 minimal cost。

假设我们存在这么一个树 T 其花费小于 $F_{n-1}$ 的代价。Pick the minimal weight edge $e\in T$ that is not in $F_{n-1}$. Adding e to $F_{n-1}$ introduces a unique cycle C in $F_{n-1}$. This cycle has some strange properties. First, e has the highest cost of ant edgh on C. For otherwise, Kruskal's algorithm would hace chosen it before the heavier weight edges. Second, there is another edge in C that's not in T(because T was a tree it can't have the entire cycle). Call such an edge e'. Now we can remove e' from $F_{n-1}$ and add e. This can only increase the total cost of $F_{n-1}$, but this transformation produces a tree with one more edge in common with T than before. This contradicts that T had strictly lower weight than $F_{n-1}$, because repeating the process we described would eventually transform $F_{n-1}$ into T exactly, while only increasing the total cost.

## Columns of Matrices
假设具有一个类似这样的矩阵
$$A\ =\ 
\begin{matrix}
    2 & 0 & 1 & -1 & 0 \\
    0 & -4 & 0 & 1 & 0 \\
    0 & 0 & 1 & 0 & 7 \\
\end{matrix}
$$
In the standard interpretation of linear algebra, this matrix represents a linear function f from one vector space V to another W, with the basis $(v_1, \dots, v_5)$ of V being represented by columns and the basis $(w_1, w_2, w_3)$ of W being represented by the rows.

现在考虑计算矩阵的秩。

## The Matriod
**Definition**: Let X be a finite set. An independence system over X is a family J of subsets of X with the following properties.
1. J is nonempty.
2. If $I\in J$, then so is every subset of I.

**Definition**: A matriod M = ( X , J ) is a set X and an independence system J over X with the following property:  
If A, B are in J with |A| = |B| + 1, then there is an element $x\in A\ or\ B$ such that $B\cup{a}\in J$

换句话说，这个性质说明，如果我有一个相互独立（线性无关）的集合，并且这个集合不是最大线性无关集合，那么就可以在更大的集合中找到一个合适的元素，将该元素加进之前的集合，通过这种方式来扩充之前的集合。我们把这种性质叫做 扩展特性 （extension property）

For a warmup exercise, let's prove that the extension property is equivalent to the following

    For every subset Y in X, all maximal independent sets contained in Y have equal size.


**Theorem** Suppose that $M = (X, \mathscr{I})$ is an independence system, and that we have a black-box algorithm to determine whether a given set is independent. Define the greedy algorithm to iteratively adds the cheapest element of X that maintains independence. Then the greedy algorithm produces a maximally independent set S of minimal cost for every nonnegative cost function on X, if and only if M is a matroid.

It’s clear that the algorithm will produce a set that is maximally independent. The only question is whether what it produces has minimum weight among all maximally independent sets. We’ll break the theorem into the two directions of the “if and only if”:

Part 1: If M is a matroid, then greedy works perfectly no matter the cost function.  
Part 2: If greedy works perfectly for every cost function, then M is a matroid.

所以，只要能够证明面临的问题是一个 matroid 那么就可以使用贪心算法。